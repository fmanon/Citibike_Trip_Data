{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Database\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citi Bike first launched in New York City back in May of 2013 with 6,000 bikes total.  Since then, it has grown to 12,000 bikes shared among 143,000 members (as of May 2018).  Citi Bike's makes its data available for all.  Given the availability of said data, I plan on doing the following:\n",
    "\n",
    "1. Build a SQL database populated with the data\n",
    "2. Build visualization tools that would help tell a story\n",
    "3. Build a time series model that would help with estimating future data\n",
    "\n",
    "For this project, we will be using all available Citi Bike Jersey City trip data through December 31st, 2019.  The data can be found [here](https://s3.amazonaws.com/tripdata/index.html).  We will start off by building a webscraper to pull all the necessary data.  After downloading the data, we will combine them into a PostgreSQL database, which we will be able to query in a later notebook in order to extract any relevant data needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [01. Importing Libraries](#01.-Importing-Libraries)\n",
    "- [02. Data Prep](#02.-Data-Prep)\n",
    "- [03. Building a PostgreSQL Database](#03.-Building-a-PostgreSQL-Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 01. Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Downloading, moving and unzipping files\n",
    "import webbrowser\n",
    "from time import sleep\n",
    "import shutil\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# DataFrame exploration and manipulation\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# PostgreSQL interaction\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 02. Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial attempt, the link where all the data files were stored, https://s3.amazonaws.com/tripdata/index.html, returns a page which we cannot work with.  After some digging around and manipulating the url link, we ended up omitting the trailing 'index.html' piece to return a more workable page format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://s3.amazonaws.com/tripdata/'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A response of 200 is the greenlight we need to proceed.  Next we will instantiate our `soup` variable to look for the tag that we should leverage to obtain the links to each downloadable zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'xml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through what was returned above (as well as plugging in the url into a web browser, we can see that the .zip files that we want are stored with a `<Key>` tag.  So now we know where to look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = soup.find_all('Key')\n",
    "# data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to retreive all of the zip files from the site.  We will do this by first instantiating an empty list in order to store the proper file names found in `data_files`.  Afterwards, we will iterate through each list item and leverage the `webbrowser` library to open each respective download link.  This process runtime depends on how long the `sleep` function is used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty list\n",
    "zip_files = []\n",
    "\n",
    "# Populate list with zip file names\n",
    "for file in range(len(data_files)-1):\n",
    "    zip_files.append(data_files[file].get_text())\n",
    "\n",
    "# Download Jersey City zip files\n",
    "for file in zip_files:\n",
    "    if 'JC' in file:\n",
    "        webbrowser.open_new(url + file)\n",
    "        sleep(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading all of the respective files, we will unzip their contents and then relocate them from the `/Downloads` folder to the `../data` of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'C:/Users/Fausto/Downloads/'\n",
    "destination = 'C:/Users/Fausto/Documents/github_fmanon/Citibike_Trip_Data/data/'\n",
    "\n",
    "# Unzip files and clean up data folder\n",
    "for item in os.listdir(source):\n",
    "    if item.endswith('.zip'):\n",
    "        file_name = source + item\n",
    "        zip_ref = ZipFile(file_name)\n",
    "        zip_ref.extractall(source)\n",
    "        zip_ref.close()\n",
    "        os.remove(file_name)\n",
    "\n",
    "# Move from Download folder to data folder\n",
    "for item in os.listdir(source):\n",
    "        shutil.move(source + item, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take a look at details for some files, just to make sure everything lines up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6668 entries, 0 to 6667\n",
      "Data columns (total 15 columns):\n",
      "Trip Duration              6668 non-null int64\n",
      "Start Time                 6668 non-null object\n",
      "Stop Time                  6668 non-null object\n",
      "Start Station ID           6668 non-null int64\n",
      "Start Station Name         6668 non-null object\n",
      "Start Station Latitude     6668 non-null float64\n",
      "Start Station Longitude    6668 non-null float64\n",
      "End Station ID             6668 non-null int64\n",
      "End Station Name           6668 non-null object\n",
      "End Station Latitude       6668 non-null float64\n",
      "End Station Longitude      6668 non-null float64\n",
      "Bike ID                    6668 non-null int64\n",
      "User Type                  6668 non-null object\n",
      "Birth Year                 5292 non-null float64\n",
      "Gender                     6668 non-null int64\n",
      "dtypes: float64(5), int64(5), object(5)\n",
      "memory usage: 781.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/JC-201509-citibike-tripdata.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19728 entries, 0 to 19727\n",
      "Data columns (total 15 columns):\n",
      "tripduration               19728 non-null int64\n",
      "starttime                  19728 non-null object\n",
      "stoptime                   19728 non-null object\n",
      "start station id           19728 non-null int64\n",
      "start station name         19728 non-null object\n",
      "start station latitude     19728 non-null float64\n",
      "start station longitude    19728 non-null float64\n",
      "end station id             19728 non-null int64\n",
      "end station name           19728 non-null object\n",
      "end station latitude       19728 non-null float64\n",
      "end station longitude      19728 non-null float64\n",
      "bikeid                     19728 non-null int64\n",
      "usertype                   19728 non-null object\n",
      "birth year                 19728 non-null int64\n",
      "gender                     19728 non-null int64\n",
      "dtypes: float64(4), int64(6), object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('../data/JC-201912-citibike-tripdata.csv')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, despite the columns being similar in name and data types, we need to rename all of the columns to match the same format.  This is because in order for `panda`'s concatenate method to work properly, all columns should be in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('..//data/')\n",
    "\n",
    "for csv in files:\n",
    "    df = pd.read_csv(f'../data/{csv}')\n",
    "    df = df.rename(columns=({'Trip Duration':'tripduration',\n",
    "                             'Start Time':'starttime',\n",
    "                             'Stop Time':'stoptime',\n",
    "                             'Start Station ID':'start station id',\n",
    "                             'Start Station Name':'start station name',\n",
    "                             'Start Station Latitude':'start station latitude',\n",
    "                             'Start Station Longitude':'start station longitude',\n",
    "                             'End Station ID':'end station id',\n",
    "                             'End Station Name':'end station name',\n",
    "                             'End Station Latitude':'end station latitude',\n",
    "                             'End Station Longitude':'end station longitude',\n",
    "                             'Bike ID':'bikeid',\n",
    "                             'User Type':'usertype',\n",
    "                             'Birth Year':'birth year',\n",
    "                             'Gender':'gender'}))\n",
    "    df.to_csv(f'../data/{csv}', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have formatted all column headers across all downloaded data, we will now merge all `.csv` files into one for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_files = sorted(glob('../data/JC-*******citibike-tripdata.csv'))\n",
    "jc_trip_data = pd.concat((pd.read_csv(file) for file in jc_files), ignore_index = True)\n",
    "jc_trip_data.to_csv('../data/jc_trip_data.csv', index = False)\n",
    "\n",
    "for items in jc_files:\n",
    "    os.remove(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the data we just combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1403216 entries, 0 to 1403215\n",
      "Data columns (total 15 columns):\n",
      "tripduration               1403216 non-null int64\n",
      "starttime                  1403216 non-null object\n",
      "stoptime                   1403216 non-null object\n",
      "start station id           1403216 non-null int64\n",
      "start station name         1403216 non-null object\n",
      "start station latitude     1403216 non-null float64\n",
      "start station longitude    1403216 non-null float64\n",
      "end station id             1403216 non-null int64\n",
      "end station name           1403216 non-null object\n",
      "end station latitude       1403216 non-null float64\n",
      "end station longitude      1403216 non-null float64\n",
      "bikeid                     1403216 non-null int64\n",
      "usertype                   1402719 non-null object\n",
      "birth year                 1358974 non-null float64\n",
      "gender                     1403216 non-null int64\n",
      "dtypes: float64(5), int64(5), object(5)\n",
      "memory usage: 160.6+ MB\n"
     ]
    }
   ],
   "source": [
    "jc_trip_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will find the data dictionary that will help us understand the purpose each column has.  The details were created using information pulled from [Citi Bike's website](https://www.citibikenyc.com/system-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Description|\n",
    "|---|---|---|\n",
    "|tripduration|int|trip duration in seconds|\n",
    "|starttime|object|start time of the trip, including the date|\n",
    "|stoptime|object|stop time of the trip, including the date|\n",
    "|start station id|int|the station id of where the trip started|\n",
    "|start station name|object|the station name of where the trip started|\n",
    "|start station latitude|float|the latitude coordinate of where the trip started|\n",
    "|start station longitude|float|the longitude coordinate of where the trip started|\n",
    "|end station id|int|the station id of where the trip ended|\n",
    "|end station name|object|the station name of where the trip ended|\n",
    "|end station latitude|float|the latitude coordinate of where the trip ended|\n",
    "|end station longitude|float|the longitude coordinate of where the trip ended|\n",
    "|bikeid|int|the id number of the bike used during the trip|\n",
    "|usertype|object|Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member|\n",
    "|birth year|float|birth year of user on the trip|\n",
    "|gender|int| 0=unknown; 1=male; 2=female|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have formatted all of our data properly, the next section will be dedicated to building out a database in PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 03. Building a PostgreSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will establish a connection with PostgreSQL and create a new database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully in PostgreSQL \n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL\n",
    "connection = psycopg2.connect(\"user=postgres password='password'\");\n",
    "connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT);\n",
    "\n",
    "# Obtain a DB Cursor\n",
    "cursor = connection.cursor();\n",
    "db_name = \"citibike_data\";\n",
    "\n",
    "# Create DB in PostgreSQL\n",
    "create_database = f\"CREATE DATABASE {db_name};\"\n",
    "cursor.execute(create_database);\n",
    "print(\"Database created successfully in PostgreSQL \")\n",
    "\n",
    "# Closing PostgreSQL connection\n",
    "if(connection):\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a table for the Jersey City data and then populate it with the data found in `jc_trip_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully in PostgreSQL \n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Building a table for Jersey City data\n",
    "try:\n",
    "    connection = psycopg2.connect(user = 'postgres',\n",
    "                                  password = 'password',\n",
    "                                  host = '127.0.0.1',\n",
    "                                  port = '5432',\n",
    "                                  database = 'citibike_data')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Some data types have been amended below to account for all data in the csv (i.e. blank cells)\n",
    "    create_table_query = '''CREATE TABLE jersey_city(\n",
    "                         trip_duration INT,\n",
    "                         start_time TIMESTAMP,\n",
    "                         stop_time TIMESTAMP,\n",
    "                         start_station_id INT,\n",
    "                         start_station_name TEXT,\n",
    "                         start_station_latitude FLOAT,\n",
    "                         start_station_longitude FLOAT,\n",
    "                         end_station_id INT,\n",
    "                         end_station_name TEXT,\n",
    "                         end_station_latitude FLOAT,\n",
    "                         end_station_longitude FLOAT,\n",
    "                         bike_id INT,\n",
    "                         user_type TEXT,\n",
    "                         birth_year TEXT,\n",
    "                         gender INT); '''\n",
    "    \n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    print(\"Table created successfully in PostgreSQL \")\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while creating PostgreSQL table:\", error)\n",
    "finally:\n",
    "    #closing database connection.\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the table has been created, there are multiple ways to upload our csv data into the database.  One method is using `INSERT` from the `psycopg2` library.  Using this method is slightly inefficient, since we will ultimately have to loop through each row in the csv file.  A better way is to use `copy_from`.  With `copy_from`, we do not need to itereate through each row in the csv file - which is convenient, as the JC data is over 1.4 million rows - 1.4 million rows that we now don't have to loop through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully in PostgreSQL \n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Populating the Jersey City table\n",
    "try:\n",
    "    connection = psycopg2.connect(user = 'postgres',\n",
    "                                  password = 'password',\n",
    "                                  host = '127.0.0.1',\n",
    "                                  port = '5432',\n",
    "                                  database = 'citibike_data')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    with open('../data/jc_trip_data.csv', 'r') as data:\n",
    "        next(data) # Skip the header row\n",
    "        cursor.copy_from(data, 'jersey_city', sep=',')\n",
    "        \n",
    "    connection.commit()\n",
    "    \n",
    "    print(\"Table updated successfully in PostgreSQL \")\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error :\n",
    "    print (\"Error while updating PostgreSQL table:\", error)\n",
    "    \n",
    "finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have created, and populated, our PostgreSQL database.  The next notebook will be dedicated to creating queries to pull data that will help us generate useful insight into the world of Citi Bike in Jersey City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
